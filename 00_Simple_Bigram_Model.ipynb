{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054e73dc-0ba0-4af3-b808-16c1585ef580",
   "metadata": {},
   "source": [
    "# Simple Bigram Language Model\n",
    "Following the example by Andrej Karpathy, we build a simple character-level language model to **generate** text. The model is a single layer NN that recieves a single token/character as input and then predicts the next token/character.\n",
    "\n",
    "The data used for training this generative model is [\"Tiny Shakespeare\"](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt), which is a concatenation of all of the works of Shakespear's (in a single file) -- about 1MB of data.\n",
    "\n",
    "### Key concepts:\n",
    "1. Bigram: a sequence of two adjacent elements in a sequence of tokens, such as words, letters, or other units of text. \n",
    "2. batch_size: How many independent sequences will be processed in parallel.\n",
    "3. block_size: max sequence length (aka context) that will be used for prediction.\n",
    "\n",
    "## The plan\n",
    "1. Load and inspect the data\n",
    "2. Define the vocabulary, which is the total number of unique tokens in a given corpus. Here, we tokenize at the character level, where each character in a piece of text is a separate token. We end up with $65$ tokens.\n",
    "3. Create mappings from tokens to indices (encode) and visa versa (decode).\n",
    "4. Split data into train / validate sets. Given we are building a generative model that uses previous text to generate the next set of tokens, we can't assume the tokens are i.i.d., so we don't shuffle before the split.\n",
    "6. Define a `BigramLM` model to learn the token embeddings matrix (which is actually a weights matrix). After training, the weights can be used to predict the next token/character, and generate text sequences. Here, the weights (embeddings matrix elements) are in fact the bigram probabilities, so the embeddings matrix is of shape $65 \\times 65$.\n",
    "7. Inspect some generated text output(s) before training the model (from randomly initiated weights).\n",
    "8. Train the model. Generate text giving a single token input. Inspect the generated text after training.\n",
    "\n",
    "## Results\n",
    "After training, the generated sequences begin to have some of the characteristics of the Shakespearian text -- we observe words and paragraphs. So training definitely improved predictions, but it has a ways to go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61df901f-ba53-47a3-bf0a-37caebc98170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x106d94a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bccc6fa8-8bef-482b-ae75-8db9641b4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9621a2bd-d827-449b-9f85-2d4fdf50246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb179890-6512-4812-88b1-4db22c728fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "Vocab size is: 65\n"
     ]
    }
   ],
   "source": [
    "# get sorted unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(f\"\\nVocab size is: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abd5a04b-2bc7-4b35-b8b7-eb54d532f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mappings from characters to integers and visa versa\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "assert stoi[\"!\"] == 2\n",
    "assert stoi[\"z\"] == 64\n",
    "assert itos[2] == \"!\"\n",
    "assert itos[64] == \"z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe05a9b4-da62-42dc-bf3c-cddfa3b686f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode('Welcome home!') --> [35, 43, 50, 41, 53, 51, 43, 1, 46, 53, 51, 43, 2]\n",
      "decode(encode('Welcome home!')) --> Welcome home!\n"
     ]
    }
   ],
   "source": [
    "# turn a string into a list of integers\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "# turn a list of integers into a string\n",
    "decode = lambda L: \"\".join([itos[i] for i in L])\n",
    "\n",
    "print(f\"encode('Welcome home!') --> {encode('Welcome home!')}\")\n",
    "print(f\"decode(encode('Welcome home!')) --> {decode(encode('Welcome home!'))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57aa7ffa-19ef-40e9-882f-40cb55795b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) <built-in method type of Tensor object at 0x1391fb4f0>\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text and store in a torch tensor\n",
    "data = torch.tensor(encode(text))\n",
    "print(data.shape, data.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "375df127-482d-42cb-8f5d-8dbfa15e7e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d60ff1d-404b-489d-b296-5172c21e7a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# decode requires a list input\n",
    "print(decode(data[:100].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b4c3a68-46b7-450c-aa43-fafe55cbe969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train / validate sets\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16221b08-c1dc-491c-bfdc-394366ffd0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "First Cit\n"
     ]
    }
   ],
   "source": [
    "# set blocksize\n",
    "torch.manual_seed(1337)\n",
    "block_size = 8\n",
    "print(data[:block_size+1])\n",
    "print(decode(data[:block_size+1].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4534a21f-723d-4587-b433-46f4a7a39fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: tensor([18]), target: 47\n",
      "context: tensor([18, 47]), target: 56\n",
      "context: tensor([18, 47, 56]), target: 57\n",
      "context: tensor([18, 47, 56, 57]), target: 58\n",
      "context: tensor([18, 47, 56, 57, 58]), target: 1\n",
      "context: tensor([18, 47, 56, 57, 58,  1]), target: 15\n",
      "context: tensor([18, 47, 56, 57, 58,  1, 15]), target: 47\n",
      "context: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target: 58\n"
     ]
    }
   ],
   "source": [
    "# For illustration only:\n",
    "# auto-regressive behavior where every previous token in a block is part of the input\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"context: {context}, target: {target}\")\n",
    "\n",
    "# However, that's not what we work with in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29de0ff3-28a1-40c5-bd00-18400d178d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size=4\n",
    "block_size=8\n",
    "# focus on input-output where the input is the previous token\n",
    "def get_batch(split):\n",
    "    \"\"\"Generate a small batch of data (inputs x and associated targets y)\n",
    "    from the train set or validation set.\n",
    "\n",
    "    Parameters:\n",
    "        split (str): A string indicating whether to use 'train' or 'validate' data split.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two torch tensors:\n",
    "            - x (torch.Tensor): A tensor containing input data.\n",
    "            - y (torch.Tensor): A tensor containing target data.\n",
    "    \"\"\"\n",
    "    df = train_data if \"train\" else val_data\n",
    "    # randomly pick the start of the batch_size (e.g. 4) batches in the data\n",
    "    ix = torch.randint(0, len(df) - block_size, (batch_size,))\n",
    "    x = torch.stack([df[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([df[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(xb)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f4359-0ccb-48b5-ad10-ed7eae8d5274",
   "metadata": {},
   "source": [
    "## Define BigramLM model\n",
    "`BigramLM` is a character-level language model that primarily relies on an embedding layer as its input layer and computes logits as part of its output. Its primary purpose is to learn and estimate bigram transition probabilities between characters in text data and generate text based on those probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c94d31f2-a662-4149-8677-5516150f9b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 5, 9])\n",
      "tensor([[ 0.6258,  0.0255,  0.9545,  0.0643, -0.5024, -0.2026, -1.5671, -1.0980,\n",
      "          0.2360, -0.2398, -0.9211,  1.5433, -0.3676, -0.7483, -0.1006,  0.7307,\n",
      "         -2.0371,  0.4931,  1.4870,  0.5910],\n",
      "        [ 0.3404,  1.1685, -0.6526,  0.3768,  0.1209,  2.5418, -0.6405, -1.9740,\n",
      "         -1.1572,  0.2896,  0.6164, -0.4370,  0.1670,  0.4586, -1.7662,  0.5860,\n",
      "          0.5873,  0.2861,  0.0083, -0.2523],\n",
      "        [ 0.8475,  0.0774,  0.5433, -0.8438, -0.7864,  0.2444, -0.9812, -0.0699,\n",
      "          0.2984, -0.7264, -0.3119, -0.4560,  1.8354,  1.4473, -0.7374,  0.2485,\n",
      "          0.5042,  0.8713, -0.2742, -0.7469]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example to illustrate torch.nn.Embedding\n",
    "\n",
    "# The list of tokens\n",
    "tokens = torch.tensor([0,5,9], dtype=torch.long)\n",
    "# Define an embedding layer, where you know upfront that in total you\n",
    "# have 10 distinct words, and you want each word to be encoded with\n",
    "# a 20 dimensional vector\n",
    "embedding = torch.nn.Embedding(num_embeddings=10, embedding_dim=20)\n",
    "# Obtain the embeddings for each of the words in the sentence\n",
    "embedded_output = embedding(tokens)\n",
    "print(tokens)\n",
    "print(embedded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54b411c6-de3a-43ae-8bb8-ad8e5539c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = torch.nn.Embedding(num_embeddings=vocab_size,\n",
    "                                                        embedding_dim=vocab_size)\n",
    "\n",
    "    def forward(self, idx, y=None):\n",
    "        # both idx and y are of size batch_size x block size (aka B x T)\n",
    "        # logits is of size (B x T x C), where C is number of classes\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        B, T, C = logits.shape \n",
    "        if y == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # torch.nn.functional.cross_entropy requires size (N, C)\n",
    "            logits = logits.view(B*T, C)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        return logits, loss\n",
    "\n",
    "    # will use a model to generate in the general case when we use\n",
    "    # more previous tokens to generate the next; but in this example\n",
    "    # we're obviously using only a single token to generate the next\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''Take the idx sequence which is (B, T) and extend it\n",
    "        sequentially in the time dimention to (B, T+1), ..., (B, T+max_new_tokens)\n",
    "        '''\n",
    "        for _ in range(max_new_tokens):\n",
    "            # idx is (B, T) array of indices \n",
    "            logits, _ = self(idx)\n",
    "            # forcus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # In theory, the sum of class probabilities should be one for each batch\n",
    "            # However, multinomial does not require probs to sum to one (in which case it uses the values as weights)\n",
    "            # Generate next token:\n",
    "            # don't simply output the class with highest probability; generate from a multinomial distribution\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63955e22-9cdf-4bbc-a077-0b0be4a6a24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([32, 65])\n",
      "Loss before training: 4.360136985778809\n",
      "A generated sequence:\n",
      "\n",
      "cwgn,qwYfgBWd'CLI TNla-YQQCfm-nBerjt:.djA;.Q!X&i$g Pvlb BSvfBf\n",
      "krTX$d?XazHws-lo?-aqKyGOZX;rYi,qf$dDO\n"
     ]
    }
   ],
   "source": [
    "# inspect output before training.\n",
    "model = BigramLM(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(f\"Logits shape: {logits.shape}\") # (batch_size * block_size, vocab_size)\n",
    "print(f\"Loss before training: {loss.item()}\")\n",
    "# a single batch\n",
    "kickoff_index = torch.zeros((1,1), dtype=torch.long)\n",
    "out = model.generate(kickoff_index, max_new_tokens=100)[0]\n",
    "print(\"A generated sequence:\")\n",
    "print(decode(out.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1babfcc3-e997-4111-bc25-70f7b6770710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequences:\n",
      "\n",
      ";L'yvCf$Bn.BAmoscV&W;PX pQ:LnHNt!yKOA;-,,,TFJXpJ\n",
      "aIv rrKE!uXpFP3CENq\n",
      "tLu?l wUnjxerlRBdYYK -Pb$K,3HsE\n",
      "\n",
      "Vd!i,DNUlt:B&NLR!tLhP$x:RV&jErZH?jEpS&WZSKplQCOIU.Ine!!!MgBqL\n",
      "AJmR:-Ikh:;D-nczTfQEf'UcBdTfAeqK ZX;b \n",
      "\n",
      "m&bQLsAx:dtm'rkjnae\n",
      ".IfQrg kC-d,kJj:okq;tjECwv ZHxHT!lyvKETi,;,LQY';c;oMT.B .:vzxkgugE:WIlhLZmw:pPln\n",
      "\n",
      "3arrgOEIvKyvhrkZWH.BRarrVR ZsWd;bdfapSuu-j\n",
      "gVx VJrAvf\n",
      "utGEYzd'lvxsUBWdJtsDiZYBoerrlsD'lNtmvHVzE;GHpH\n"
     ]
    }
   ],
   "source": [
    "# generate a batch of 4 sequences starting with the same toekn, each with max_new_tokens=100\n",
    "# notice that we get 4 different results (because we 'generate' from a distribution)\n",
    "kickoff_index = torch.zeros((4,1), dtype=torch.long)\n",
    "out = model.generate(kickoff_index, max_new_tokens=100)\n",
    "print(\"Generated sequences:\")\n",
    "print(decode(out[0].tolist()))\n",
    "print(decode(out[1].tolist()))\n",
    "print(decode(out[2].tolist()))\n",
    "print(decode(out[3].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "101a7064-f0f3-4aef-8f88-3475944537b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 4.5606770515441895\n",
      "Epoch: 500, Loss: 4.118853569030762\n",
      "Epoch: 1000, Loss: 3.6763458251953125\n",
      "Epoch: 1500, Loss: 3.3370871543884277\n",
      "Epoch: 2000, Loss: 3.0861566066741943\n",
      "Epoch: 2500, Loss: 2.9228947162628174\n",
      "Epoch: 3000, Loss: 2.79687237739563\n",
      "Epoch: 3500, Loss: 2.5935275554656982\n",
      "Epoch: 4000, Loss: 2.5961170196533203\n",
      "Epoch: 4500, Loss: 2.612950563430786\n",
      "Epoch: 5000, Loss: 2.553447723388672\n",
      "Epoch: 5500, Loss: 2.478346347808838\n",
      "Epoch: 6000, Loss: 2.410762310028076\n",
      "Epoch: 6500, Loss: 2.5325160026550293\n",
      "Epoch: 7000, Loss: 2.498384952545166\n",
      "Epoch: 7500, Loss: 2.523305654525757\n",
      "Epoch: 8000, Loss: 2.537670135498047\n",
      "Epoch: 8500, Loss: 2.3776967525482178\n",
      "Epoch: 9000, Loss: 2.3792474269866943\n",
      "Epoch: 9500, Loss: 2.4181370735168457\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = BigramLM(vocab_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94bb0cf7-ed0c-4d1a-b5a5-e84e5b1c9d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Angee dal whayo l, chDoure han lfr t cif y ter hitsh!\n",
      "N irir theenofo's d k thapa, e eow heane loun wise mig t nofowaginthe nd t owixphenggherun Hathe.\n",
      "LUSlows, noterd Whatu y my weller Y dld hed o's:\n",
      "ENGoworru,\n",
      "Cat; kesherew'YCllllleareMERKICHqund ithar'liconfrn, me NCLieas ixtoukss ug; l.\n",
      "O duragr\n"
     ]
    }
   ],
   "source": [
    "# generate 4 sequences from trained model\n",
    "generated_idx = model.generate(kickoff_index, max_new_tokens=300)\n",
    "print(decode(generated_idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e42e8136-83c3-42c3-a7d7-d515610a5826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Murysurm, wifa ick bsoothe,\n",
      "GBwerdwirun,\n",
      "bu IZZULit, n'd pend an by t pr ingand?\n",
      "Ber'dy th bl m at he t th sthind t loburisthard I thene bl omes mpid st. bichthare w w, fany IOLELOUSerecheld, alir's? aty pitenf wst htrord,\n",
      "GLO: cbrise anvethithiencorthaplanp, derhes me s yallofan'eleayond,\n",
      "Towor bal \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decode(generated_idx[1].tolist()), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be1aa936-1a75-4ac4-ba84-cfbf79ac4df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DUCind Cofatole youblird-ctontincr y!\n",
      "HA.\n",
      "\n",
      "Doud\n",
      "IIOn m; untceen aninhis shy pe'sshen ise he y mis at warmsleeabed, od,\n",
      "S s f a g ge yzeserilathouc-hour kishe 's, selathey h IIURIUShomiorcolpeane a fu avistisotocathe l,\n",
      "\n",
      "Y my;-\n",
      "Jap BWiven arous ghenevet vesue ind w shethos.\n",
      "CLUCl.\n",
      "I for dinet t be k \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decode(generated_idx[2].tolist()), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ce10266-9ef7-4aa4-be1f-4794ea8131e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IZAnvent I f thunoce hethay he stesveating s ma ng jathad\n",
      "S:\n",
      "BEEOR:\n",
      "Wrth me bl toucenombetheing situr turesuchian t\n",
      "TO: hisgalotind coullle,\n",
      "Then fofue, latrathowe bd POre ber awinRITEd dg oupithe itthennee be bu par touimen,\n",
      "Whinthaye,\n",
      "TI mice. l plaMO:\n",
      "Tine u er touste le ayoffut gon, pllay fef nd \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decode(generated_idx[3].tolist()), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
